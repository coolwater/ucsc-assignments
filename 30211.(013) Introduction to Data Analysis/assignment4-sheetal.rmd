---
title: "Assignment 4 - Linear Regression"
author: "Sheetal Gangakhedkar"
date: "May 26, 2017"
output:
  html_document:
    toc: yes
    toc_depth: 5
  pdf_document: default
  word_document:
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment4 - Linear Regression
* Perform linear regression on Boston Housing Data. 
* Analyze the data. do some statistical analysis.
* Divide the data into training and testing.
* Calculate RMSE for test data.
* Suggest which features are important and how will you decide them.

The data consist of 506 observations and 14 non constant independent variables. The variables are listed here along with their meaning:

* crim - per capita crime rate by town.
* zn - proportion of residential land zoned for lots over 25,000 sq. ft.
* indus - proportion of non-retain business acres per town.
* chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* nox - nitrogen oxides concentration (parts per million).
* rm - average number of rooms per dwelling.
* age - proportion of owner-occupied units built prior to 1940.
* dis - weighted mean of distances to five Boston employment centers.
* rad - index of accessibility to radial highways
* tax - full-value property-tax rate per $10,000
* ptratio - pupil-teacher ratio by town
* black - 1000(Bk - 0.63)^2, where Bk is the proportion of blacks by town.
* lstat - lower status of the population (percent).
* medv - median value of owner-occupied homes in $1000s.

### Single Feature Linear Regression
```{r}
library(MASS)
data(package="MASS", "Boston")
dim(Boston)
head(Boston)
summary(Boston)
medv.lstat.model = lm(medv ~ lstat, data=Boston)
plot(medv.lstat.model)
summary(medv.lstat.model)
coef(medv.lstat.model)
confint(medv.lstat.model, level=0.95)
summary(medv.lstat.model)$coefficients
confidenceIntrvl = predict(object=medv.lstat.model, newdata=data.frame(lstat = c(5, 10, 15)), interval="confidence")
confidenceIntrvl
predictIntrvl = predict(object=medv.lstat.model, newdata=data.frame(lstat = c(5, 10, 15)), interval="predict")
predictIntrvl
library(ggplot2)
# plot residuals vs fitted values
plot(medv ~ lstat, data=Boston, pch = 20)
# increase width of the regression line by 3
abline(medv.lstat.model, lwd=3, col="red")
```

## Selecting the Best Regression Variables
To select the best subset of regression variables, the step function can perform forward or backward stepwise regression and come up with the optimal set of predictors and model.

### Backward Stepwise Regression
Backward stepwise regression starts with many variables and removes the underperformers.
```{r}
# start with a model that includes all the predictors, called the full model
full.model = lm(medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data=Boston)
# dump the summary of the full model
summary(full.model)
# eliminate the insignificant variables using step function to create reduced model
reduced.model = step(full.model, direction="backward")
summary(reduced.model)
```
### Forward Stepwise Regression
Forward stepwise regression starts with a few variables and adds new ones to improve the model until it cannot be improved any further.

```{r}
min.model = lm(medv ~ 1, data=Boston)
fwd.model = step(min.model, direction="forward", scope=( ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat), trace=0)
summary(fwd.model)
```

### Results Analysis
The Backward stepwise regression using step removed 'indus', and 'age' predictors as insignificant or underperformers.

* indus - proportion of non-retail business acres per town.
* age - proportion of owner-occupied units built prior to 1940.

#### Residuals statistics

Min: -15.5984, 1Q: -2.7386, Median: -0.5046, 3Q: 1.7273, Max: 26.2373
 
The sign of the median '-0.5046' indicates the skew's direction, and the magnitude of the median indicates the extent. In this case the median is negative, which suggests some skew
to the left. The residuals have a nice, bell-shaped distribution, around the left skewed median, (1Q: -2.7386  Median: -0.5046   Q3: 1.7273), as the Q1 and Q3 quartiles have the same magnitude around the median.

#### Are the coefficients significant? (significant features)
The column labeled Estimate contains the estimated regression coefficients as calculated
by ordinary least squares.Theoretically, if a variable's coefficient is zero then the variable is worthless; it adds nothing to the model.We therefore ask: Statistically speaking, how likely is it that the true coefficient is zero? That is the purpose of the t statistics and the p-values, which in the summary are labeled (respectively) t value and Pr(>|t|).

p-value is the probability that gauges the likelihood that a coefficient is not
significant, so smaller is better. In the reduced model, the p-value does not exceed 0.001551  (for 'chas') for any of the features, and it is quite lower than the conventional limit of 0.05.

Based on the p-value, significant features/predictors for 'medv' in the Boston data model are:

```{r}
attr(terms(reduced.model), "predvars")
```


#### R flags
A handy feature is that R flags the significant variables for quick identification. All the reduced model predictors/features have '**' or '***', which indicates their significance.

#### Residual standard error
Residual standard error: '4.736 on 494 degrees of freedom'. This reports the standard error of the residuals (??)-that is, the sample standard deviation of ??.

#### Is the model useful? (R2 - coefficient of determination))
Multiple R-squared:  0.7406, Adjusted R-squared:  0.7348

R2 is a measure of the model's quality. Bigger is better. Mathematically, it is the
fraction of the variance of response 'medv' that is explained by the regression model. The remaining variance is not explained by the model, so it must be due to other factors (i.e., unknown variables or sampling variability). In this case, the model explains 0.7406
(74.06%) of the variance of 'medv', and the remaining 0.2594 (25.94%) is unexplained.

It is strongly suggested using the adjusted rather than the basic R2. The adjusted value accounts for the number of variables in the model and so it is a more realistic assessment of its effectiveness. In this case, it is better to use 0.7348, not 0.7406.

The column labeled Std. Error is the standard error of the estimated coefficient. The column labeled t value is the t statistic from which the p-value was calculated.

#### Is the model statistically significant? (F Statistic)
F-statistic: 128.2 on 11 and 494 DF,  p-value: < 2.2e-16

Conventionally, a p-value of less than 0.05 indicates that the model is likely significant
(one or more ??i are nonzero) whereas values exceeding 0.05 indicate that
the model is likely not significant. Here, the probability is only 2.2e-16 that our
model is insignificant. That's very good.

## Plotting Regression Residuals
Normally, plotting a regression model object produces several diagnostic plots. You can select just the residuals plot by specifying which=1.
```{r}
plot(reduced.model, which=1)
plot(reduced.model, which=2)
```

## Root Mean Squared Error (RMSE)

The square root of the mean/average of the square of all of the error. The use of RMSE is very common and it makes an excellent general purpose error metric for numerical predictions. Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors.

RMSE = sqrt( mean( (sim - obs)^2, na.rm = TRUE) )

```{r}
library(ModelMetrics)

# we dont need these columns, as they are not significant 
Boston$age = NULL
Boston$indus = NULL

# score the model, by using the orignal data as testing data
predicted = predict(reduced.model, Boston)

#get the actual values 
actual = Boston$medv

# calculated RMSE
rmse(actual, predicted)

# the reported RMSE
summary(reduced.model)$sigma
```
